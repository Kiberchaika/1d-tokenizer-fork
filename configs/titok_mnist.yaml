experiment:
    tokenizer_checkpoint: "tokenizer_titok_mnist.bin"
    generator_checkpoint: "generator_titok_mnist.bin"
    output_dir: "titok_mnist"

model:
    vq_model:
        codebook_size: 256  # Reduced due to simpler dataset
        token_size: 8  # Reduced due to smaller image size
        use_l2_norm: True
        commitment_cost: 0.25
        # vit arch
        vit_enc_model_size: "small"  # Reduced model size
        vit_dec_model_size: "small"
        vit_enc_patch_size: 4  # Smaller patch size for 28x28 images
        vit_dec_patch_size: 4
        num_latent_tokens: 16  # Fewer tokens needed for MNIST
        inputcolors: 1

    generator:
        model_type: "ViT"
        hidden_size: 256  # Reduced hidden size
        num_hidden_layers: 8  # Fewer layers
        num_attention_heads: 8  # Fewer attention heads
        intermediate_size: 1024  # Smaller intermediate size
        dropout: 0.1
        attn_drop: 0.1
        num_steps: 8
        mask_schedule_strategy: "arccos"
        class_label_dropout: 0.1
        image_seq_len: ${model.vq_model.num_latent_tokens}
        condition_num_classes: 10  # MNIST has 10 classes

        # sampling hyper-params
        randomize_temperature: 4.0  # Reduced temperature
        guidance_scale: 2.0  # Reduced guidance scale
        guidance_decay: "linear"

dataset:
    preprocessing:
        crop_size: 28  # MNIST images are 28x28

training:
    batch_size: 32  # Reduced batch size
    accumulation_steps: 4  # Gradient accumulation
    use_mixed_precision: True